## Introduction
A census analytics system is a platform designed to process, analyze and generate insights from census data. Census data in this case refers to the demographic information retrieved from a population at regular intervals, typically conducted by governments to gather information about the population's characteristics, such as age, gender, race, income, education level, household composition, and more.
The census analytics system has proved efficient where census officials can upload census data from households enumeration and insights are generated after the processing operations on this data. To narrow this done in terms of deliverables, this project seeks to analyze and calculate the minimum and maximum income and family size for each occupation and store the results in a GridDB database leveraging on stacks of technologies such as Java, Hadoop, JDBC, and GridDB.

## How to Follow Along
You can grab the source code from our repo:

`$ git clone https://github.com/victortosin02/census

## Technologies Used:
The following stack of technologies were leveraged on to efficiently analyze the census data:
Java for programming
- Java for programming
- Hadoop for distributed processing
- JDBC for database connectivity
- GridDB for storing and managing data

## Benefits of Census Analytics System?
This project demonstrates the power of combining Java, Hadoop, JDBC, and GridDB for efficient and scalable data analytics. The benefits of a census analytics system includes but not limited to the following:

- Improved data processing efficiency with Hadoop's distributed computing framework
- Enhanced data management using GridDB's scalable storage solution
- Flexible data analysis with JDBC for efficient querying and analysis
- Extract valuable insights into census trends and patterns using Java code

## What are the operational processes and workflow of this project?
1. **Data Ingestion:** Read census data from a CSV file.
2. **Data Processing:** Use Hadoop to calculate minimum and maximum income and family size for each occupation.
3. **Data Storage:** Store processed data in GridDB database using JDBC.
4. **Data Analysis:** Retrieve and analyze data from GridDB to gain insights into census trends.

## What Weâ€™re Building
In this guide, we will embark on a journey to create a census analytics system capable of ingesting, processing and analyzing census data to generate valuable insights that will help in proper decision making. This system aim to empower stakeholders to properly plan and allocate resources efficiently.

## Prerequisites
What you need to install:
- Java version: 17
- Maven version: 3.*
- Hadoop version: 3.2.4(apache distribution)

You can download Java from their website: https://www.java.com/en/download/
You can download Hadoop from their website: https://hadoop.apache.org/releases.html
You can download Hadoop from their website: https://griddb.net/en/downloads/


## Project Overview
The Census Analytics System is a comprehensive platform designed to process, analyze, and generate insights from census data. Leveraging a stack of technologies including Java, Hadoop, JDBC, and GridDB, this project offers a scalable and efficient solution for census data management and analysis. 

The Census Analytics System offers a robust and efficient solution for processing and analyzing census data. By leveraging Java, Hadoop, JDBC, and GridDB, we are able to derive valuable insights from census data, enabling data-driven decision-making and policy formulation. With its scalable architecture and powerful analytical capabilities, the system serves as a valuable tool for researchers, policymakers, and businesses seeking to understand and address demographic challenges.


## Setting up the Project
- Create a new Maven project.
- Add necessary dependencies to the `pom.xml` file.
 XML
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.1</version>
    </dependency>
    <dependency>
        <groupId>com.toshiba.griddb</groupId>
        <artifactId>gridstore</artifactId>
        <version>4.3.0</version>
    </dependency>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.12</version>
        <scope>test</scope>
    </dependency>
</dependencies>

## Data Ingestion
Our CensusDataMapper class will read the input data and emit the occupation as the key and the income and family size as the value:

public class CensusDataMapper extends Mapper<LongWritable, Text, Text, Text> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        // Ignore header line starting with @
        if (line.startsWith("@")) {
            return;
        }
        String[] parts = line.split(",");
        if (parts.length != 4) {
            // Skip malformed input
            return;
        }
        String occupation = parts[2].trim();
        String income = parts[3].trim();
        String familySize = parts[1].trim();
        // Emit occupation as key and income,familySize as value
        context.write(new Text(occupation), new Text(income + "," + familySize));
    }
}

## Data Analytics
Our CensusDataReducer class will calculate the minimum and maximum income and family size for each occupation:

public class CensusDataReducer extends Reducer<Text, Text, Text, Text> {
    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        int minIncome = Integer.MAX_VALUE;
        int maxIncome = Integer.MIN_VALUE;
        int minFamilySize = Integer.MAX_VALUE;
        int maxFamilySize = Integer.MIN_VALUE;
        for (Text value : values) {
            String[] parts = value.toString().split(",");
            int income = Integer.parseInt(parts[0]);
            int familySize = Integer.parseInt(parts[1]);
            minIncome = Math.min(minIncome, income);
            maxIncome = Math.max(maxIncome, income);
            minFamilySize = Math.min(minFamilySize, familySize);
            maxFamilySize = Math.max(maxFamilySize, familySize);
        }
        context.write(key, new Text(minIncome + "," + maxIncome + "," + minFamilySize + "," + maxFamilySize));
    }
}

## Data Processing
Our CensusDataAnalyzerJob class will set up the Hadoop job and run it:

public class CensusDataAnalyzerJob implements HadoopJob {
    @Override
    public boolean runJob(JobParams jobParams) {
        try {
            // Create job
            Job job = createJob();
            // Set Mapper and Reducer
            setMapperAndReducer(job);
            // Set input/output format classes
            setInputAndOutputFormatClass(job);
            // Set input/output paths
            setInputOutputPaths(job, jobParams);
            job.setJarByClass(AppMain.class);
            job.setJobName("Census Data Analysis: Min-Max");
            job.setWorkingDirectory(new Path(Path.CUR_DIR));
            // Submit the job to the cluster and wait for it to finish
            return job.waitForCompletion(true);
        } catch (Exception ex) {
            ex.printStackTrace();
        }
        return false;
    }
    // ...
}

AppMain
Our AppMain class will run the Hadoop job:

public class AppMain {
    public static void main(String[] args) {
        JobParams jobParams = new JobParams("input", "output");
        if (args.length == 2) {
            System.out.println("Using custom job params.");
            jobParams.setInputPath(args[0]);
            jobParams.setOutputPath(args[1]);
        } else {
            System.out.println("Usage: $hadoop jar target/census-data-analysis-1.0.jar input output");
            System.exit(1);
        }
        boolean success = new CensusDataAnalyzerJob().runJob(jobParams);
        if (!success) {
            System.exit(1);
        }
    }
}


## Data Storage
Setting up JDBC for GridDB
- Add GridDB JDBC dependency to pom.xml file

<dependency>
    <groupId>com.toshiba.griddb</groupId>
    <artifactId>griddb-jdbc</artifactId>
    <version>4.3.0</version>
</dependency>

- Create griddb.properties file with GridDB configuration
griddb.username=admin
griddb.password=admin
griddb.url=jdbc:griddb://localhost:31999/
griddb.database=census_data

- Create GridDBConnection class to establish JDBC connection

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;

public class GridDBConnection {
    private static Connection connection;

    public static Connection getConnection() {
        if (connection == null) {
            try {
                Class.forName("com.toshiba.griddb.jdbc.GridDBDriver");
                connection = DriverManager.getConnection(
                        "jdbc:griddb://" + System.getProperty("griddb.url") + "/",
                        System.getProperty("griddb.username"),
                        System.getProperty("griddb.password")
                );
            } catch (SQLException | ClassNotFoundException e) {
                e.printStackTrace();
            }
        }
        return connection;
    }
}

- Modify CensusDataReducer class to store processed data in GridDB

public class CensusDataReducer extends Reducer<Text, Text, Text, Text> {
    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        // ...
        Connection connection = GridDBConnection.getConnection();
        try (Statement statement = connection.createStatement()) {
            statement.execute("INSERT INTO census_data (occupation, min_income, max_income, min_family_size, max_family_size) VALUES ('" + key.toString() + "', " + minIncome + ", " + maxIncome + ", " + minFamilySize + ", " + maxFamilySize + ")");
        } catch (SQLException e) {
            e.printStackTrace();
        }
    }
}

Now, the processed data will be stored in the census_data table in the GridDB database.


## Conclusion:
Census Data Analytics using Java, Hadoop, JDBC, and GridDB is a powerful project that demonstrates the strengths of each technology in handling large and complex datasets. By leveraging these technologies, organizations can extract valuable insights from census data, informing decision-making and driving business success.